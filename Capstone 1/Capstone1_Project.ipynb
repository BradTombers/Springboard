{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "During the year between May 2014 and May 2015, over 20,000 homes were bought and sold in King County, the most populous county in Washingon state.  The most notable city is Seattle, which in 2015 had a population of about 675,000.  It's home to major tech corporations such as Microsoft and Amazon.  The local government collected data from home purchases and made these data available on their website. This notebook discusses a method for predicting home prices using the nineteen features found in the King County public dataset.  \n",
    "\n",
    "## The Dataset\n",
    "The King County Housing Dataset was scraped from King County's public data portal and posted to the data website Kaggle.com by user harlfoxem. It can be found at this link: https://www.kaggle.com/harlfoxem/housesalesprediction/data. Before sharing the data on Kaggle, user hrlfoxem cleaned it to provide users a smooth entry into practicing linear regression techniques.  The dataset has nineteen features, plus the date sold and unique id for each home.  The features are as follows:\n",
    "\n",
    "|Feature Name|Description|\n",
    "|-|\n",
    "|id|unique home identifier|\n",
    "|date|date house was sold|\n",
    "|price|price is prediction target|\n",
    "|bedrooms|number of bedrooms|\n",
    "|bathrooms|number of bathrooms|\n",
    "|sqft_living|square footage of the home|\n",
    "|sqft_lot|square footage of the lot|\n",
    "|floors|total floors (levels in house|\n",
    "|waterfront|indicates whether the home is positioned on the water|\n",
    "|view|indicates whether the home has been viewed|\n",
    "|condition|rating of the overall condition of the home|\n",
    "|grade|overall grade given to the housing unit based on King County grading system|\n",
    "|sqft_above|square footage of house apart from basement|\n",
    "|sqft_basement|square footage of the basement|\n",
    "|yr_built|year the home was built|\n",
    "|yr_renovated|year the home was renovated|\n",
    "|zipcode|zipcode the home resides in|\n",
    "|lat|latitude coordinate|\n",
    "|long|longitude coordinate|\n",
    "|sqft_living15|living room area in 2015 (implies some renovations) -this might or might not have affected the lotsize area|\n",
    "|sqft_lot15|lot size area in 2015 (implies some renovations)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  sqft_living  \\\n",
       "0  7129300520  20141013T000000  221900.0         3       1.00         1180   \n",
       "1  6414100192  20141209T000000  538000.0         3       2.25         2570   \n",
       "2  5631500400  20150225T000000  180000.0         2       1.00          770   \n",
       "3  2487200875  20141209T000000  604000.0         4       3.00         1960   \n",
       "4  1954400510  20150218T000000  510000.0         3       2.00         1680   \n",
       "\n",
       "   sqft_lot  floors  waterfront  view     ...      grade  sqft_above  \\\n",
       "0      5650     1.0           0     0     ...          7        1180   \n",
       "1      7242     2.0           0     0     ...          7        2170   \n",
       "2     10000     1.0           0     0     ...          6         770   \n",
       "3      5000     1.0           0     0     ...          7        1050   \n",
       "4      8080     1.0           0     0     ...          8        1680   \n",
       "\n",
       "   sqft_basement  yr_built  yr_renovated  zipcode      lat     long  \\\n",
       "0              0      1955             0    98178  47.5112 -122.257   \n",
       "1            400      1951          1991    98125  47.7210 -122.319   \n",
       "2              0      1933             0    98028  47.7379 -122.233   \n",
       "3            910      1965             0    98136  47.5208 -122.393   \n",
       "4              0      1987             0    98074  47.6168 -122.045   \n",
       "\n",
       "   sqft_living15  sqft_lot15  \n",
       "0           1340        5650  \n",
       "1           1690        7639  \n",
       "2           2720        8062  \n",
       "3           1360        5000  \n",
       "4           1800        7503  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 847,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import matplotlib.ticker as mtick\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(\"kc_house_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over rows checking for any missing values \n",
    "sum([True for idx,row in df.iterrows() if any(row.isnull())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no rows with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this time, we have determined that the data needs no more pre-processing. 21,613 different home purchases are available to use in our model. Lets start with a look at the distribution of home prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#investigate price column \n",
    "fmt = '${x:,.0f}'\n",
    "tick = mtick.StrMethodFormatter(fmt)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 9))\n",
    "\n",
    "ax1 = fig.add_subplot(2,1,1)\n",
    "ax1.hist(df.price, bins = 300, color = 'xkcd:grey')\n",
    "ax1.set_title(\"Home Prices in King County\")\n",
    "ax1.xaxis.set_major_formatter(tick) \n",
    "\n",
    "ax2 = fig.add_subplot(2,1,2)\n",
    "ax2.hist(df.price, bins = 500, color = 'xkcd:grey')\n",
    "ax2.set_xlim(0,1000000)\n",
    "ax2.set_title(\"Home Prices in King County up to $1,000,000\")\n",
    "ax2.xaxis.set_major_formatter(tick) \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A correlation matrix is a figure that allows us to see the relationship between each of the features in our dataset.  The sns heatmap() function allows us to visaualize with color those variables that are highly correlated to each other.  We'll be focusing specifically on the correlates to price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.corr()['price'].sort_values(ascending = False)[1:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20))\n",
    "sns.heatmap(df.corr(),cmap='RdGy',linewidths=0.1,linecolor='white',annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top correlates to home price are:\n",
    "\n",
    "Feature | Correlation\n",
    "--- | ---\n",
    "sqft_living | 0.70 <br>\n",
    "grade | 0.67<br>\n",
    "sqft_above | 0.61<br>\n",
    "sqft_living15 | 0.59<br>\n",
    "bathrooms | 0.53<br>\n",
    "view | 0.40<br>\n",
    "\n",
    "There are also strong correlations between the variables that have to do with square footage.  For example, a home with a large square footage is likely to also have a large value for \n",
    "square footage upstairs and downstairs.  Also, the number of bedrooms is positively correlated to square footage values.  \n",
    "\n",
    "Below are scatterplots for each of the top correlates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20,20))\n",
    "fmt = '${x:,.0f}'\n",
    "tick = mtick.StrMethodFormatter(fmt)\n",
    "#fig.yaxis.set_major_formatter(tick) \n",
    "\n",
    "ax1 = fig.add_subplot(3,2,1)\n",
    "ax2 = fig.add_subplot(3,2,2)\n",
    "ax3 = fig.add_subplot(3,2,3)\n",
    "ax4 = fig.add_subplot(3,2,4)\n",
    "ax5 = fig.add_subplot(3,2,5)\n",
    "ax6 = fig.add_subplot(3,2,6)\n",
    "\n",
    "#plot price vs sqft_living\n",
    "ax1.scatter(df.sqft_above,df.price, c = 'xkcd:black', alpha = .15)\n",
    "ax1.set_xlabel('Square footage - total')\n",
    "ax1.set_ylabel('Price')\n",
    "ax1.yaxis.set_major_formatter(tick) \n",
    "\n",
    "#plot price vs grade\n",
    "ax2.scatter(df.grade,df.price, c = 'xkcd:black', alpha = .15)\n",
    "ax2.set_xlabel('Grade')\n",
    "ax2.set_ylabel('Price')\n",
    "ax2.yaxis.set_major_formatter(tick) \n",
    "\n",
    "#plot price vs sqft_above\n",
    "ax3.scatter(df.sqft_above,df.price, c = 'xkcd:black', alpha = .15)\n",
    "ax3.set_xlabel('Square footage - above basement')\n",
    "ax3.set_ylabel('Price')\n",
    "ax3.yaxis.set_major_formatter(tick) \n",
    "\n",
    "#plot price vs sqft_living15\n",
    "ax4.scatter(df.sqft_living15,df.price, c = 'xkcd:black', alpha = .15)\n",
    "ax4.set_xlabel('Square footage - living room')\n",
    "ax4.set_ylabel('Price')\n",
    "ax4.yaxis.set_major_formatter(tick) \n",
    "\n",
    "#plot price vs bathrooms\n",
    "ax5.scatter(df.bathrooms,df.price, c = 'xkcd:black', alpha = .15)\n",
    "ax5.set_xlabel('Bathrooms')\n",
    "ax5.set_ylabel('Price')\n",
    "ax5.yaxis.set_major_formatter(tick) \n",
    "\n",
    "#plot price vs view\n",
    "ax6.scatter(df.view,df.price, c = 'xkcd:black', alpha = .15)\n",
    "ax6.set_xlabel('View')\n",
    "ax6.set_ylabel('Price')\n",
    "ax6.yaxis.set_major_formatter(tick) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Each of the square footage metrics have a weak linear relationship.  The grade and bathrooms features have significant variance depending on each discrete value, and the view seems to not be a good linear fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20,15))\n",
    "\n",
    "renovated = df[df['yr_renovated'] > 0]\n",
    "not_renovated = df[df['yr_renovated'] <= 0]\n",
    "to_plot_renovated = [renovated.price, not_renovated.price]\n",
    "ax1 = fig.add_subplot(3,1,1)\n",
    "ax1.set_xlabel(\"Price\")\n",
    "ax1.xaxis.set_major_formatter(tick) \n",
    "plt.boxplot(to_plot_renovated, vert = False, widths = .25,labels = ['Renovated Homes','Non-Renovated Homes'])\n",
    "\n",
    "waterfront = df[df['waterfront'] > 0]\n",
    "not_waterfront = df[df['waterfront'] <= 0]\n",
    "to_plot = [waterfront.price, not_waterfront.price]\n",
    "ax2 = fig.add_subplot(3,1,2)\n",
    "ax2.set_xlabel(\"Price\")\n",
    "ax2.xaxis.set_major_formatter(tick)\n",
    "plt.boxplot(to_plot, vert = False, widths = .25,labels = ['Waterfront Homes','Non-Waterfront Homes'])\n",
    "\n",
    "view = df[df['view'] > 0]\n",
    "not_view = df[df['view'] <= 0]\n",
    "to_plot = [view.price, not_view.price]\n",
    "ax3 = fig.add_subplot(3,1,3)\n",
    "ax3.set_xlabel(\"Price\")\n",
    "ax3.xaxis.set_major_formatter(tick) \n",
    "plt.boxplot(to_plot, vert = False, widths = .25,labels = ['Viewed Homes','Non-Viewed Homes'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homes that have been renovated tend to have a higher value.  Seventy-five percent of non-renovated homes are less expensive than half of all renovated homes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homes on the waterfront tend to be much more expensive.  A full seventy-five percent of non-waterfront homes are less expensive than the cheapest quarter of all homes on the waterfront. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homes that have not been viewed tend to be less valuable than viewed homes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Creating a linear regression model\n",
    "\n",
    "[Linear Regression](http://en.wikipedia.org/wiki/Linear_regression) is a method to model the relationship between a set of independent variables $X$ (also knowns as explanatory variables, features, predictors) and a dependent variable $Y$.  This method assumes the relationship between each predictor $X$ is **linearly** related to the dependent variable $Y$. Multiple linear regression contains multiple independent variables.\n",
    "\n",
    "$$ Y = \\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p + \\epsilon$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.graphics.regressionplots import *\n",
    "from scipy import stats\n",
    "\n",
    "# this function returns the mean absolute percentage error between predicted and actual values\n",
    "def mean_absolute_percentage_error(y_test, y_pred):\n",
    "    # return the MAPE score\n",
    "    return np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "\n",
    "# this function performs linear regression on a data set\n",
    "# input: data and target\n",
    "def make_linear_model(X, y, plot = True):\n",
    "    # create train and test sets with 70/30 split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 23)\n",
    "    \n",
    "    # fit the train data and create predictions with test data\n",
    "    reg = linear_model.LinearRegression()\n",
    "    reg.fit(X_train, y_train)\n",
    "    y_pred = reg.predict(X_test)\n",
    "    \n",
    "    # create dataframe with features and coefficients\n",
    "    results = pd.DataFrame({'Features': X.columns, 'Estimated Coefficients': reg.coef_})[['Features', 'Estimated Coefficients']]\n",
    "    \n",
    "    # print results and summary statistics\n",
    "    print(results)\n",
    "    print('\\nEstimated intercept coefficient: {}'.format(reg.intercept_))    \n",
    "    print('\\nSummary Statistics\\nR-squared value: {}'.format(reg.score(X_test, y_test)))\n",
    "    print(\"Root Mean Squared Error: {}\".format(np.sqrt(mean_squared_error(y_test,y_pred))))\n",
    "    print(\"Mean Absolute Percentage Error (MAPE): {}\".format(mean_absolute_percentage_error(y_test,y_pred)))\n",
    " \n",
    "    if plot == True:        \n",
    "        \n",
    "        fig = plt.figure(figsize = (18,5))    \n",
    "        # plot residuals\n",
    "        ax1 = fig.add_subplot(1,3,1)\n",
    "        plt.scatter(y_pred, y_pred - y_test, c = 'xkcd:grey', marker = '.', s = 40)\n",
    "        plt.hlines(y=0, xmin = -50000, xmax = 3750000)\n",
    "        ax1.set_xlim(0,y_pred.max())\n",
    "        ax1.tick_params(rotation = 35)\n",
    "        ax1.xaxis.set_major_formatter(tick) \n",
    "        ax1.yaxis.set_major_formatter(tick)\n",
    "        ax1.set_title('Residual plot using test data')\n",
    "        ax1.set_ylabel('Residuals')\n",
    "\n",
    "        # plot probability plot \n",
    "        ax2 = fig.add_subplot(1,3,2)  \n",
    "        stats.probplot(y_pred - y_test, plot = plt)\n",
    "\n",
    "\n",
    "        # plot true prices vs predicted prices\n",
    "        # fig2, ax3 = plt.subplots(figsize = (8,7))\n",
    "        ax3 = fig.add_subplot(1,3,3)\n",
    "        plt.scatter(y_test, y_pred,c = 'xkcd:grey', alpha = .25)\n",
    "        ax3.set_xlabel(\"True prices\")\n",
    "        ax3.set_ylabel(\"Predicted Prices\")\n",
    "        ax3.xaxis.set_major_formatter(tick) \n",
    "        ax3.yaxis.set_major_formatter(tick)\n",
    "        ax3.set_title(\"True prices compared to predicted prices\")\n",
    "        ax3.tick_params(rotation = 35)\n",
    "        ax3.set_xlim(0,y_pred.max())\n",
    "        ax3.set_ylim(0,y_pred.max())\n",
    "        plt.plot([0, y_pred.max()], [0, y_pred.max()],c = 'xkcd:black',linestyle='-', linewidth=2)\n",
    "\n",
    "        plt.subplots_adjust(wspace = .35)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model using all features\n",
    "The python package StatsModels gives a nice output with many statistics, so we'll use it to gain an overall understanding of the features we're working with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['price','id','date'], axis = 1)\n",
    "y = df['price'].values\n",
    "\n",
    "est = sm.OLS(y, X)\n",
    "fitted = est.fit()\n",
    "print(fitted.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the output, we can already see that the feature \"floors\" is not a useful indicator of home price, due to it's high P-value.  We'll remove it from the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Success\n",
    "Our goal is to create a model that can accurately forecast home prices.  One common metric we can use is the mean absolute percentage error (MAPE).  It quantifies how far off the model is in absolute terms.  Going forward, I'll be using the MAPE score to assess the quality of each model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "make_linear_model(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using my custom function, we can see that the MAPE score using all features is __25.83%__.  This is our baseline that we will attempt to improve upon.  From the plots above, we can see that the residuals are not evenly distributed, which indicates some non-linear factors.  Also, the probability plot shows some skewedness on both ends. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "In order to minimize the MAPE, we'll first cluster the data before applying a model. We'll cluster the data only on the most impactful features, price, sqft_living, and grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "df_cluster = df[['price','sqft_living','grade']]\n",
    "\n",
    "# initalize a KMeans instance and fit it to the dataframe\n",
    "km = KMeans(n_clusters = 3, init = 'k-means++', n_init = 10)\n",
    "km.fit(df_cluster)\n",
    "x = km.fit_predict(df_cluster)\n",
    "\n",
    "# create new column indicating which cluster the observation belongs to\n",
    "df['cluster'] = x\n",
    "\n",
    "# split dataframe based on clustering\n",
    "# from doing the clustering many times, I determined that the clusters will be split into roughly 15,000, 6,000, and 600 \n",
    "# observations.  To remain consistent, I'll assign them titles in the order of\n",
    "# cluster 0 being the smallest group and cluster2 being the largest.\n",
    "\n",
    "if len(df.loc[df.cluster == 0].index) < 1000:\n",
    "    cluster0 = df.loc[df.cluster == 0]\n",
    "elif len(df.loc[df.cluster == 1].index) < 1000:\n",
    "    cluster0 = df.loc[df.cluster == 1]\n",
    "elif len(df.loc[df.cluster == 2].index) < 1000:\n",
    "    cluster0 = df.loc[df.cluster == 2]\n",
    "    \n",
    "if (len(df.loc[df.cluster == 0].index) > 1000) & (len(df.loc[df.cluster == 0].index) < 10000):\n",
    "    cluster1 = df.loc[df.cluster == 0]\n",
    "elif (len(df.loc[df.cluster == 1].index) > 1000) & (len(df.loc[df.cluster == 1].index) < 10000):\n",
    "    cluster1 = df.loc[df.cluster == 1]   \n",
    "elif (len(df.loc[df.cluster == 2].index) > 1000) & (len(df.loc[df.cluster == 2].index) < 10000):\n",
    "    cluster1 = df.loc[df.cluster == 2]   \n",
    "    \n",
    "if len(df.loc[df.cluster == 0].index) > 10000:\n",
    "    cluster2 = df.loc[df.cluster == 0]\n",
    "elif len(df.loc[df.cluster == 1].index) > 10000:\n",
    "    cluster2 = df.loc[df.cluster == 1]\n",
    "elif len(df.loc[df.cluster == 2].index) > 10000:\n",
    "    cluster2 = df.loc[df.cluster == 2]\n",
    "\n",
    "print(cluster0.shape)\n",
    "print(cluster1.shape)\n",
    "print(cluster2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Clusters\n",
    "We can see that the data is now clustered into three groups, of varied size.  Below is a look at the distribution of the prices of the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = [cluster0.price, cluster1.price, cluster2.price]\n",
    "\n",
    "fig = plt.figure(figsize = (20,15))\n",
    "ax = fig.add_subplot(3,1,1)\n",
    "ax.set_xlabel(\"Price\")\n",
    "ax.xaxis.set_major_formatter(tick) \n",
    "plt.boxplot(to_plot, vert = False, widths = .25,labels = ['Cluster 0','Cluster 1','Cluster 2'])\n",
    "\n",
    "to_plot = [cluster0.sqft_living, cluster1.sqft_living, cluster2.sqft_living]\n",
    "\n",
    "ax2 = fig.add_subplot(3,1,2)\n",
    "ax2.set_xlabel(\"Square Footage of Home\")\n",
    "plt.boxplot(to_plot, vert = False, widths = .25,labels = ['Cluster 0','Cluster 1','Cluster 2'])\n",
    "\n",
    "to_plot = [cluster0.grade, cluster1.grade, cluster2.grade]\n",
    "\n",
    "ax3 = fig.add_subplot(3,1,3)\n",
    "ax3.set_xlabel(\"Grade\")\n",
    "plt.boxplot(to_plot, vert = False, widths = .25,labels = ['Cluster 0','Cluster 1','Cluster 2'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 0, which has about 600 observations, seems to be a grouping of the higher priced homes.  The homes in this group tend to have higher square footage, and grade values as well. There are several outliers at the high end, but most prices fall between about $1.5 and 3 million.  \n",
    "\n",
    "Cluster 1 has about 6,000 observations, and contains homes priced at about $500,000 to the lower bounds of Cluster 0. The square footage and grade are in the middle of clusters 0 and 2.\n",
    "\n",
    "Cluster 2 has the most observations, with 15,000.  It's comprised of the lowest priced homes, with the smallest square footage, and mostly lower grades.  It makes sense that most homes in this cluster are sold for under a million dollars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Encoding\n",
    "\n",
    "To improve the predictive power of my model, I'm using feature encoding. To eliminate some of the variability of a feature, we can group each observation into intervals of similarly priced homes.  After each observation is in it's bucket, we calculate the average of each bucket, and then assign that average value to a new column in the dataframe.  In a way, it's similar to clustering, but using only one specific variable.  \n",
    "\n",
    "For example, when I encode the feature 'sqft_living', each home will be grouped into 30 equally populated buckets corresponding to their square footage.  I'll calculate the average of each bucket, and it will be assigned to a new feature, which can be used to predict the price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_feature(df, feature_name, num_buckets = 30):\n",
    "    \n",
    "    # create name for new feture, and copy the dataframe\n",
    "    bucket = feature_name + '_bucket_interval'\n",
    "    bucket_price = feature_name + '_bucket_price'\n",
    "    df_copy = df.copy()\n",
    "        \n",
    "    # create a new feature that contains the interval to which the observation belongs\n",
    "    df_copy[bucket] = pd.qcut(df[feature_name],num_buckets, duplicates = 'drop')\n",
    "   \n",
    "    # group the intervals together and calculate the mean price for each interval/bucket\n",
    "    df_new = df_copy.groupby(df_copy[bucket], as_index = False).agg({'price':'mean'})\n",
    "    df_new = pd.DataFrame(df_new)\n",
    "    \n",
    "    # specify the column names, and create a row that contains the number of the bucket for plotting\n",
    "    df_new.rename(columns = {'price':bucket_price}, inplace = True)\n",
    "    df_new['bucket_number'] = df_new.index + 1\n",
    "    df_new.head()\n",
    "    \n",
    "    # show a scatter plot of the buckets\n",
    "    fig, ax = plt.subplots(figsize = (8,6))\n",
    "    plt.scatter(df_new.bucket_number,df_new[bucket_price])\n",
    "    plt.xlabel('Bucket Number - ' + feature_name)\n",
    "    plt.ylabel('Average price')\n",
    "    ax.yaxis.set_major_formatter(tick) \n",
    "    plt.show()\n",
    "    \n",
    "    #join df and df_new on the new bucket_interval feature\n",
    "    df_join = pd.merge(df_new, df_copy, on=bucket)\n",
    "    df_join.drop([bucket, 'bucket_number'], axis = 1, inplace = True)\n",
    "    return df_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create encoded features\n",
    "cluster0 = encode_feature(cluster0,'sqft_living')\n",
    "cluster0 = encode_feature(cluster0, 'grade')\n",
    "cluster0 = encode_feature(cluster0, 'bathrooms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graphs, the encoded features for sqft_living, grade, and bathrooms seem to have a quadratic relationship with price.  No other variables showed clear relationships after encoding.  In order to capture the quadratic relationship, we'll create new features by squaring the encoded value for each observation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster0['sqft_living_bucket_price_squared'] = cluster0['sqft_living_bucket_price'] ** 2\n",
    "cluster0['grade_bucket_price_squared'] = cluster0['grade_bucket_price'] ** 2\n",
    "cluster0['bathrooms_bucket_price_squared'] = cluster0['bathrooms_bucket_price'] ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating a model for Cluster 0, we will look at the top correlates to guide our feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print the top ten correlates to price -  index 1-11 because index 0 is price itself, which has a correlation of 1.\n",
    "print(cluster0.corr()['price'].sort_values(ascending = False)[1:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['sqft_living_bucket_price_squared','waterfront','condition','grade', 'sqft_living']\n",
    "X = cluster0[features]\n",
    "y = cluster0.price\n",
    "\n",
    "make_linear_model(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A MAPE score of __17.36%__ is a vast improvement over our original score of __25.85%__.  The distribution of the residuals is somewhat normal, but there are signs of skewedness at the edges as seen in the probability plot.  The table below describes all combination of features tested with their corresponding MAPE scores.\n",
    "\n",
    "Features Used | MAPE\n",
    "--- | ---\n",
    "sqft_living_bucket_price_squared, waterfront, condition, grade, sqft_living | 17.360 <br>\n",
    "sqft_living_bucket_price_squared, bathrooms, waterfront, condition, grade, sqft_living | 17.365 <br>\n",
    "sqft_living_bucket_price_squared, bathrooms, waterfront, condition, grade, yr_built, sqft_living | 17.477<br>\n",
    "sqft_living_bucket_price_squared, bathrooms, waterfront, condition, grade, yr_built, sqft_living, sqft_living15 | 17.506<br>\n",
    "bathrooms_bucket_price, grade_bucket_price_squared,  sqft_living_bucket_price_squared, bathrooms, waterfront, condition, grade, yr_built, sqft_living, sqft_living15 | 17.686<br>\n",
    "sqft_above, bathrooms_bucket_price, grade_bucket_price_squared, sqft_living_bucket_price_squared, bathrooms, waterfront, condition, grade, yr_built, sqft_living, sqft_living15 | 17.623<br>\n",
    "sqft_above, bathrooms_bucket_price_squared, grade_bucket_price_squared, sqft_living_bucket_price_squared, bathrooms, waterfront, condition, grade, yr_built, sqft_living, sqft_living15 | 17.683<br>\n",
    "bathrooms_bucket_price_squared, grade_bucket_price_squared, sqft_living_bucket_price_squared, bathrooms, waterfront, condition, grade, yr_built, sqft_living, sqft_living15 | 17.705<br>\n",
    "grade_bucket_price_squared,  sqft_living_bucket_price_squared, bathrooms, waterfront, condition, grade, yr_built, sqft_living, sqft_living15 | 17.743<br>\n",
    "sqft_living_bucket_price_squared, sqft_living, bathrooms_bucket_price, bathrooms, grade_bucket_price_squared, sqft_above, grade | 19.19<br>\n",
    "\n",
    "To ensure the features we selected are statistically signifcant, we'll run a statsmodels linear regression and look at the P-values for each feature.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = sm.OLS(y, X)\n",
    "fitted = est.fit()\n",
    "print(fitted.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column labeled 'P>|t|' shows us that each of the features we selected are statistically significant influencers of price.  We can be satisfied with this model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create encoded features\n",
    "cluster1 = encode_feature(cluster1,'sqft_living')\n",
    "cluster1 = encode_feature(cluster1, 'grade')\n",
    "cluster1 = encode_feature(cluster1, 'bathrooms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graphs, the encoded features for sqft_living, grade, and bathrooms seem to have either a linear or a quadratic relationship with price.  No other variables showed clear relationships after encoding.  In order to capture the quadratic relationship, we'll create new features by squaring the encoded value for each observation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster1['sqft_living_bucket_price_squared'] = cluster1['sqft_living_bucket_price'] ** 2\n",
    "cluster1['grade_bucket_price_squared'] = cluster1['grade_bucket_price'] ** 2\n",
    "cluster1['bathrooms_bucket_price_squared'] = cluster1['bathrooms_bucket_price'] **2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating a model for Cluster 1, we will look at the top correlates to guide our feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the top ten correlates to price -  index 1-11 because index 0 is price itself, which has a correlation of 1.\n",
    "print(cluster1.corr()['price'].sort_values(ascending = False)[1:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['bathrooms_bucket_price_squared','grade_bucket_price','bathrooms','waterfront','condition','grade','yr_built','zipcode','lat','long', 'sqft_living','sqft_living15']\n",
    "X = cluster1[features]\n",
    "y = cluster1.price\n",
    "\n",
    "make_linear_model(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A MAPE score of __12.94%__ is a vast improvement over our original score of __25.85%__.  The distribution of the residuals is troubling, but the diagonal pattern seen is caused by the clustering.  This cluster seems to have a strict cut-off point near $600,000.  This causes our probability plot to show a wave-like pattern.  Because this is caused by the clustering, I'm less concerned about it than I otherwise would be.  The table below describes all combination of features tested with their corresponding MAPE scores.\n",
    "\n",
    "Features Used | MAPE\n",
    "--- | ---\n",
    "bathrooms_bucket_price, grade_bucket_price, sqft_living_bucket_price_squared, bathrooms, waterfront, condition, grade, yr_built, zipcode, lat, long, sqft_living, sqft_living15 | 13.116 <br>\n",
    "bathrooms_bucket_price, grade_bucket_price_squared, sqft_living_bucket_price_squared, bathrooms, waterfront, condition, grade, yr_built, zipcode, lat, long, sqft_living, sqft_living15 | 13.120 <br>\n",
    "bathrooms_bucket_price_squared, grade_bucket_price, bathrooms, waterfront, condition, grade, yr_built, zipcode, lat, long, sqft_living, sqft_living15 | 13.123 <br>\n",
    "bathrooms_bucket_price_squared, grade_bucket_price, sqft_living_bucket_price_squared, bathrooms, waterfront, condition, grade, yr_built, zipcode, lat, long, sqft_living, sqft_living15 | 13.123 <br>\n",
    "grade_bucket_price_squared, sqft_living_bucket_price_squared, bathrooms, waterfront, condition, grade, yr_built, zipcode, lat, long, sqft_living, sqft_living15 | 13.214 <br>\n",
    "bathrooms_bucket_price, grade_bucket_price_squared, sqft_living_bucket_price_squared, bathrooms, waterfront, condition, grade, yr_built, zipcode, lat, long, sqft_living | 13.270 <br>\n",
    "bathrooms_bucket_price, grade_bucket_price_squared, sqft_living_bucket_price_squared, bathrooms, waterfront, condition, grade, yr_built, sqft_living, sqft_living15 | 13.941 <br>\n",
    "bathrooms_bucket_price_squared, sqft_living, grade_bucket_price_squared, grade, sqft_above, bathrooms_bucket_price, bathrooms | 14.67 <br>\n",
    "\n",
    "To ensure the features we selected are statistically signifcant, we'll run a statsmodels linear regression and look at the P-values for each feature.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = sm.OLS(y, X)\n",
    "fitted = est.fit()\n",
    "print(fitted.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column labeled 'P>|t|' shows us that each of the features we selected are statistically significant influencers of price, since they all fall below the arbitrary value of .05.  We can be satisfied with this model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encode features\n",
    "cluster2 = encode_feature(cluster2,'sqft_living')\n",
    "cluster2 = encode_feature(cluster2, 'grade')\n",
    "cluster2 = encode_feature(cluster2, 'bathrooms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graphs, the encoded features for sqft_living, grade, and bathrooms seem to have either a linear or a quadratic relationship with price.  Grade may be related by the root function.  No other variables showed clear relationships after encoding.  In order to capture the quadratic relationship, we'll create new features by squaring the encoded value for each observation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster2['sqft_living_bucket_price_squared'] = cluster2['sqft_living_bucket_price'] ** 2\n",
    "cluster2['grade_bucket_price_sqrt'] = cluster2['grade_bucket_price'] ** .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the top ten correlates to price -  index 1-11 because index 0 is price itself, which has a correlation of 1.\n",
    "print(cluster2.corr()['price'].sort_values(ascending = False)[1:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['bedrooms', 'grade_bucket_price_sqrt', 'sqft_living_bucket_price_squared', 'bathrooms', 'waterfront', 'condition', 'grade', 'yr_built', 'zipcode', 'lat', 'long', 'sqft_living' ]\n",
    "X = cluster2[features]\n",
    "y = cluster2.price\n",
    "\n",
    "make_linear_model(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A MAPE score of __18.03%__ is a vast improvement over our original score of __25.85%__.  The distribution of the residuals is normal, and as before, the diagonal pattern is caused by the clustering. The probability plot shows us that our residuals are normal. The table below describes all combination of features tested with their corresponding MAPE scores.\n",
    "\n",
    " Features Used | MAPE\n",
    "--- | ---\n",
    "grade_bucket_price, sqft_living_bucket_price, bedrooms, bathrooms, waterfront, condition, grade, yr_built, zipcode, lat, long, sqft_living | 17.913 <br>\n",
    "bathrooms_bucket_price, grade_bucket_price, sqft_living_bucket_price, bedrooms, bathrooms, waterfront, condition, grade, yr_built, zipcode, lat, long, sqft_living | 17.913 <br>\n",
    "bedrooms, grade_bucket_price_sqrt, sqft_living_bucket_price_squared, bathrooms, waterfront, condition, grade, yr_built, zipcode, lat, long, sqft_living |  17.926 <br>\n",
    "bedrooms, grade_bucket_price, sqft_living_bucket_price_squared, bathrooms, waterfront, condition, grade, yr_built, zipcode, lat, long, sqft_living |  17.927 <br>\n",
    "grade_bucket_price, sqft_living_bucket_price, bathrooms, waterfront, condition, grade, yr_built, zipcode, lat, long, sqft_living | 18.055 <br>\n",
    "grade_bucket_price, sqft_living_bucket_price, bathrooms, waterfront, condition, grade, yr_built, lat, long, sqft_living, | 18.066 <br>\n",
    "lat, grade_bucket_price_sqrt, grade, sqft_living_bucket_price_squared, sqft_living, sqft_above | 18.62 <br>\n",
    "\n",
    "\n",
    "To ensure the features we selected are statistically signifcant, we'll run a statsmodels linear regression and look at the P-values for each feature.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = sm.OLS(y, X)\n",
    "fitted = est.fit()\n",
    "print(fitted.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column labeled 'P>|t|' shows us that each of the features we selected are statistically significant influencers of price, since they all fall below the arbitrary value of .05.  We can be satisfied with this model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "After clustering the data and encoding features, I used the mean absolute percentage error to determine my selected features.  The average improvement after clustering and feature encoding is 9.28%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations\n",
    "This data was collected for 1 year in King County, Washington.  It would be dangerous to attempt to use this model to predict anything other than a home selling in that area during that time period.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations\n",
    "For clients wishing to purchase a home in King County, Washington, they should be aware that the largest factor when determining cost is the square footage, location, and condition (grade) of the home.  At best, only about 60% of the variance is explained by the model, so the predictions should not be heavily relied upon."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
