{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "campaign_data = pd.read_csv('campaign_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['send_date'], format = \"%d-%m-%Y %H:%M\")\n",
    "df.drop('send_date', axis = 1, inplace = True)\n",
    "df.index = df['user_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date = min(df['date']) + pd.Timedelta(days = 115)\n",
    "train_df = df.loc[df['date'] < split_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BradT\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: 'user_id' is both a column name and an index level.\n",
      "Defaulting to column but this will raise an ambiguity error in a future version\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\BradT\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\BradT\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: 'user_id' is both a column name and an index level.\n",
      "Defaulting to column but this will raise an ambiguity error in a future version\n",
      "  \n",
      "C:\\Users\\BradT\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\BradT\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\BradT\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\BradT\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:2754: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_df['no_of_emails'] =train_df.groupby('user_id').size()\n",
    "train_df['cumulative_count'] = train_df.groupby('user_id').cumcount()\n",
    "\n",
    "# returns timestamp object day of week and hour of day\n",
    "def weekday(a):\n",
    "    return a.dayofweek\n",
    "\n",
    "def hourofday(a):\n",
    "    return a.hour\n",
    "\n",
    "train_df['day_of_week']= train_df['date'].apply(weekday)\n",
    "train_df['hour_of_day']= train_df['date'].apply(hourofday)\n",
    "\n",
    "train_df.fillna(0, inplace = True)\n",
    "train_df.reset_index(drop = True, inplace=True)\n",
    "\n",
    "# merge campagin data with emails\n",
    "train_df = campaign_data.merge(train_df, on = 'campaign_id')\n",
    "train_df['temp'] = 1\n",
    "\n",
    "# Calculates the count received of each type of campaign - essentially whether or not they received that campaign\n",
    "pivot_df = pd.pivot_table(train_df, values=\"temp\", index=\"user_id\", columns=\"campaign_id\", aggfunc=\"count\", fill_value=0).reset_index()\n",
    "pivot_df.columns = ['user_id'] + ['campaign_' + str(col) for col in range(29,52)]\n",
    "train_df = train_df.merge(pivot_df, on = 'user_id')\n",
    "\n",
    "# Calculates the count received of each type of communication\n",
    "pivot_df = pd.pivot_table(train_df, values=\"temp\", index=\"user_id\", columns=\"communication_type\", aggfunc=\"count\", fill_value=0).reset_index()\n",
    "pivot_df.columns = ['user_id', 'conference_count', 'corporate_count','hackathon_count','newsletter_count','others_count','upcoming_events_count','webinar_count']\n",
    "train_df = train_df.merge(pivot_df, on = 'user_id')\n",
    "\n",
    "# calculate percentage of communcation type received for each user\n",
    "train_df['conference_percent']=train_df['conference_count'] / train_df['no_of_emails']\n",
    "train_df['corporate_percent']=train_df['corporate_count'] / train_df['no_of_emails']\n",
    "train_df['hackathon_percent']=train_df['hackathon_count'] / train_df['no_of_emails']\n",
    "train_df['newsletter_percent']=train_df['newsletter_count'] / train_df['no_of_emails']\n",
    "train_df['others_percent']=train_df['others_count'] / train_df['no_of_emails']\n",
    "train_df['upcoming_events_percent']=train_df['upcoming_events_count'] / train_df['no_of_emails']\n",
    "train_df['webinar_percent']=train_df['webinar_count'] / train_df['no_of_emails']\n",
    "\n",
    "# drop unneccesary columns\n",
    "train_df.drop(['temp','conference_count', 'corporate_count','hackathon_count','newsletter_count','others_count','upcoming_events_count','webinar_count'], axis = 1, inplace = True)\n",
    "\n",
    "# NLP\n",
    "vectorizer = CountVectorizer(min_df = 1)\n",
    "\n",
    "corpus = train_df['subject']\n",
    "x = vectorizer.fit_transform(corpus)\n",
    "x.toarray()\n",
    "matrix1 = x.toarray()\n",
    "vectorizer.vocabulary_.get('harvest')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(matrix1)\n",
    "\n",
    "asd = tfidf.toarray()\n",
    "asd1 = pd.DataFrame(asd) \n",
    "features = vectorizer.get_feature_names() \n",
    "asd1.columns = features\n",
    "train_df = pd.concat((train_df,asd1), axis = 1) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df.loc[df['date'] >= split_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>campaign_id</th>\n",
       "      <th>communication_type</th>\n",
       "      <th>total_links</th>\n",
       "      <th>no_of_internal_links</th>\n",
       "      <th>no_of_images</th>\n",
       "      <th>no_of_sections</th>\n",
       "      <th>email_body</th>\n",
       "      <th>subject</th>\n",
       "      <th>email_url</th>\n",
       "      <th>id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>is_open</th>\n",
       "      <th>is_click</th>\n",
       "      <th>date</th>\n",
       "      <th>train</th>\n",
       "      <th>campaign_29</th>\n",
       "      <th>campaign_30</th>\n",
       "      <th>campaign_31</th>\n",
       "      <th>campaign_32</th>\n",
       "      <th>campaign_33</th>\n",
       "      <th>campaign_34</th>\n",
       "      <th>campaign_35</th>\n",
       "      <th>campaign_36</th>\n",
       "      <th>campaign_37</th>\n",
       "      <th>campaign_38</th>\n",
       "      <th>campaign_39</th>\n",
       "      <th>campaign_40</th>\n",
       "      <th>campaign_41</th>\n",
       "      <th>campaign_42</th>\n",
       "      <th>campaign_43</th>\n",
       "      <th>campaign_44</th>\n",
       "      <th>campaign_45</th>\n",
       "      <th>campaign_46</th>\n",
       "      <th>campaign_47</th>\n",
       "      <th>campaign_48</th>\n",
       "      <th>campaign_49</th>\n",
       "      <th>campaign_50</th>\n",
       "      <th>campaign_51</th>\n",
       "      <th>campaign_52</th>\n",
       "      <th>campaign_53</th>\n",
       "      <th>campaign_54</th>\n",
       "      <th>conference_count</th>\n",
       "      <th>corporate_count</th>\n",
       "      <th>hackathon_count</th>\n",
       "      <th>newsletter_count</th>\n",
       "      <th>others_count</th>\n",
       "      <th>upcoming_events_count</th>\n",
       "      <th>webinar_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29</td>\n",
       "      <td>Newsletter</td>\n",
       "      <td>67</td>\n",
       "      <td>61</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>Dear AVians,\\r\\n \\r\\nWe are shaping up a super...</td>\n",
       "      <td>Sneak Peek: A look at the emerging data scienc...</td>\n",
       "      <td>http://r.newsletters.analyticsvidhya.com/7um44...</td>\n",
       "      <td>29_185580</td>\n",
       "      <td>185580</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-07-01 18:01:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>Upcoming Events</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear AVians,\\r\\n \\r\\nAre your eager to know wh...</td>\n",
       "      <td>[July] Data Science Expert Meetups &amp; Competiti...</td>\n",
       "      <td>http://r.newsletters.analyticsvidhya.com/7up0e...</td>\n",
       "      <td>30_185580</td>\n",
       "      <td>185580</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-07-05 14:01:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>Others</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Fireside Chat with DJ Patil - the master is he...</td>\n",
       "      <td>[Delhi NCR] Fireside Chat with DJ Patil, Forme...</td>\n",
       "      <td>http://r.newsletters.analyticsvidhya.com/7uvlg...</td>\n",
       "      <td>33_185580</td>\n",
       "      <td>185580</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-07-24 14:51:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49</td>\n",
       "      <td>Conference</td>\n",
       "      <td>119</td>\n",
       "      <td>117</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Patron,\\r\\n \\r\\nAnalytics Vidhya is on a ...</td>\n",
       "      <td>Register @ DataHack Summit 2017 - India's Larg...</td>\n",
       "      <td>http://r.newsletters.analyticsvidhya.com/o7ohw...</td>\n",
       "      <td>49_185580</td>\n",
       "      <td>185580</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-09-28 15:02:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52</td>\n",
       "      <td>Newsletter</td>\n",
       "      <td>67</td>\n",
       "      <td>62</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>November Newsletter\\r\\n \\r\\nDear AVians,\\r\\n \\...</td>\n",
       "      <td>[Newsletter] Stage for DataHack Summit 2017 is...</td>\n",
       "      <td>http://r.newsletters.analyticsvidhya.com/7vtb2...</td>\n",
       "      <td>52_185580</td>\n",
       "      <td>185580</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-11-02 12:34:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   campaign_id communication_type  total_links  no_of_internal_links  \\\n",
       "0           29         Newsletter           67                    61   \n",
       "1           30    Upcoming Events           18                    14   \n",
       "2           33             Others            7                     3   \n",
       "3           49         Conference          119                   117   \n",
       "4           52         Newsletter           67                    62   \n",
       "\n",
       "   no_of_images  no_of_sections  \\\n",
       "0            12               3   \n",
       "1             7               1   \n",
       "2             1               1   \n",
       "3            16               1   \n",
       "4            10               4   \n",
       "\n",
       "                                          email_body  \\\n",
       "0  Dear AVians,\\r\\n \\r\\nWe are shaping up a super...   \n",
       "1  Dear AVians,\\r\\n \\r\\nAre your eager to know wh...   \n",
       "2  Fireside Chat with DJ Patil - the master is he...   \n",
       "3  Dear Patron,\\r\\n \\r\\nAnalytics Vidhya is on a ...   \n",
       "4  November Newsletter\\r\\n \\r\\nDear AVians,\\r\\n \\...   \n",
       "\n",
       "                                             subject  \\\n",
       "0  Sneak Peek: A look at the emerging data scienc...   \n",
       "1  [July] Data Science Expert Meetups & Competiti...   \n",
       "2  [Delhi NCR] Fireside Chat with DJ Patil, Forme...   \n",
       "3  Register @ DataHack Summit 2017 - India's Larg...   \n",
       "4  [Newsletter] Stage for DataHack Summit 2017 is...   \n",
       "\n",
       "                                           email_url         id  user_id  \\\n",
       "0  http://r.newsletters.analyticsvidhya.com/7um44...  29_185580   185580   \n",
       "1  http://r.newsletters.analyticsvidhya.com/7up0e...  30_185580   185580   \n",
       "2  http://r.newsletters.analyticsvidhya.com/7uvlg...  33_185580   185580   \n",
       "3  http://r.newsletters.analyticsvidhya.com/o7ohw...  49_185580   185580   \n",
       "4  http://r.newsletters.analyticsvidhya.com/7vtb2...  52_185580   185580   \n",
       "\n",
       "   is_open  is_click                date  train  campaign_29  campaign_30  \\\n",
       "0        0         0 2017-07-01 18:01:00      1            1            1   \n",
       "1        0         0 2017-07-05 14:01:00      1            1            1   \n",
       "2        0         0 2017-07-24 14:51:00      1            1            1   \n",
       "3        0         0 2017-09-28 15:02:00      1            1            1   \n",
       "4        0         0 2017-11-02 12:34:00      1            1            1   \n",
       "\n",
       "   campaign_31  campaign_32  campaign_33  campaign_34  campaign_35  \\\n",
       "0            0            0            1            0            0   \n",
       "1            0            0            1            0            0   \n",
       "2            0            0            1            0            0   \n",
       "3            0            0            1            0            0   \n",
       "4            0            0            1            0            0   \n",
       "\n",
       "   campaign_36  campaign_37  campaign_38  campaign_39  campaign_40  \\\n",
       "0            0            0            0            0            0   \n",
       "1            0            0            0            0            0   \n",
       "2            0            0            0            0            0   \n",
       "3            0            0            0            0            0   \n",
       "4            0            0            0            0            0   \n",
       "\n",
       "   campaign_41  campaign_42  campaign_43  campaign_44  campaign_45  \\\n",
       "0            0            0            0            0            0   \n",
       "1            0            0            0            0            0   \n",
       "2            0            0            0            0            0   \n",
       "3            0            0            0            0            0   \n",
       "4            0            0            0            0            0   \n",
       "\n",
       "   campaign_46  campaign_47  campaign_48  campaign_49  campaign_50  \\\n",
       "0            0            0            0            1            0   \n",
       "1            0            0            0            1            0   \n",
       "2            0            0            0            1            0   \n",
       "3            0            0            0            1            0   \n",
       "4            0            0            0            1            0   \n",
       "\n",
       "   campaign_51  campaign_52  campaign_53  campaign_54  conference_count  \\\n",
       "0            0            1            0            0                 1   \n",
       "1            0            1            0            0                 1   \n",
       "2            0            1            0            0                 1   \n",
       "3            0            1            0            0                 1   \n",
       "4            0            1            0            0                 1   \n",
       "\n",
       "   corporate_count  hackathon_count  newsletter_count  others_count  \\\n",
       "0                0                0                 2             1   \n",
       "1                0                0                 2             1   \n",
       "2                0                0                 2             1   \n",
       "3                0                0                 2             1   \n",
       "4                0                0                 2             1   \n",
       "\n",
       "   upcoming_events_count  webinar_count  \n",
       "0                      1              0  \n",
       "1                      1              0  \n",
       "2                      1              0  \n",
       "3                      1              0  \n",
       "4                      1              0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id\n",
       "5     2\n",
       "6     6\n",
       "7    12\n",
       "8    10\n",
       "9     7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = test_df.groupby('user_id').size()\n",
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BradT\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: 'user_id' is both a column name and an index level.\n",
      "Defaulting to column but this will raise an ambiguity error in a future version\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\BradT\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\BradT\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: 'user_id' is both a column name and an index level.\n",
      "Defaulting to column but this will raise an ambiguity error in a future version\n",
      "  \n",
      "C:\\Users\\BradT\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\BradT\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\BradT\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\BradT\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:2754: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'no_of_emails'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2441\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2442\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2443\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'no_of_emails'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-6a29411539e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# calculate percentage of communcation type received for each user\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'conference_percent'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'conference_count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'no_of_emails'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'corporate_percent'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'corporate_count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'no_of_emails'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hackathon_percent'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hackathon_count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'no_of_emails'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1962\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1963\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1964\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1966\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1969\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1970\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1971\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1973\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1643\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1644\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1645\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1646\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1647\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   3588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3589\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3590\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3591\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3592\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2442\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2443\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2444\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2446\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'no_of_emails'"
     ]
    }
   ],
   "source": [
    "test_df['no_of_emails'] = test_df.groupby('user_id').size()\n",
    "test_df['cumulative_count'] = test_df.groupby('user_id').cumcount()\n",
    "\n",
    "# returns timestamp object day of week and hour of day\n",
    "def weekday(a):\n",
    "    return a.dayofweek\n",
    "\n",
    "def hourofday(a):\n",
    "    return a.hour\n",
    "\n",
    "test_df['day_of_week']= test_df['date'].apply(weekday)\n",
    "test_df['hour_of_day']= test_df['date'].apply(hourofday)\n",
    "\n",
    "test_df.fillna(0, inplace = True)\n",
    "test_df.reset_index(drop = True, inplace=True)\n",
    "\n",
    "# merge campagin data with emails\n",
    "test_df  = campaign_data.merge(df, on = 'campaign_id')\n",
    "test_df['train'] = 1\n",
    "\n",
    "# Calculates the count received of each type of campaign - essentially whether or not they received that campaign\n",
    "pivot_df = pd.pivot_table(test_df , values=\"train\", index=\"user_id\", columns=\"campaign_id\", aggfunc=\"count\", fill_value=0).reset_index()\n",
    "pivot_df.columns = ['user_id'] + ['campaign_' + str(col) for col in range(29,55)]\n",
    "test_df  = test_df.merge(pivot_df, on = 'user_id')\n",
    "\n",
    "# Calculates the count received of each type of communication\n",
    "pivot_df = pd.pivot_table(test_df, values=\"train\", index=\"user_id\", columns=\"communication_type\", aggfunc=\"count\", fill_value=0).reset_index()\n",
    "pivot_df.columns = ['user_id', 'conference_count', 'corporate_count','hackathon_count','newsletter_count','others_count','upcoming_events_count','webinar_count']\n",
    "test_df = test_df.merge(pivot_df, on = 'user_id')\n",
    "\n",
    "# calculate percentage of communcation type received for each user\n",
    "test_df['conference_percent']=test_df['conference_count'] / test_df['no_of_emails']\n",
    "test_df['corporate_percent']=test_df['corporate_count'] / test_df['no_of_emails']\n",
    "test_df['hackathon_percent']=test_df['hackathon_count'] / test_df['no_of_emails']\n",
    "test_df['newsletter_percent']=test_df['newsletter_count'] / test_df['no_of_emails']\n",
    "test_df['others_percent']=test_df['others_count'] / test_df['no_of_emails']\n",
    "test_df['upcoming_events_percent']=test_df['upcoming_events_count'] / test_df['no_of_emails']\n",
    "test_df['webinar_percent']=test_df['webinar_count'] / test_df['no_of_emails']\n",
    "\n",
    "# drop unneccesary columns\n",
    "test_df .drop(['train','conference_count', 'corporate_count','hackathon_count','newsletter_count','others_count','upcoming_events_count','webinar_count'], axis = 1, inplace = True)\n",
    "\n",
    "# NLP\n",
    "vectorizer = CountVectorizer(min_df = 1)\n",
    "\n",
    "corpus = test_df['subject']\n",
    "x = vectorizer.fit_transform(corpus)\n",
    "x.toarray()\n",
    "matrix1 = x.toarray()\n",
    "vectorizer.vocabulary_.get('harvest')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(matrix1)\n",
    "\n",
    "asd = tfidf.toarray()\n",
    "asd1 = pd.DataFrame(asd) \n",
    "features = vectorizer.get_feature_names() \n",
    "asd1.columns = features\n",
    "test_df  = pd.concat((test_df ,asd1), axis = 1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(['communication_type','email_body','subject','email_url'], axis = 1, inplace = True)\n",
    "test_df.drop(['communication_type','email_body','subject','email_url'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop('date', axis = 1, inplace = True)\n",
    "test_df.drop('date', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "761657 + 261534"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = list(set(test.columns) - set(train_df.columns))\n",
    "test_df.drop(to_drop, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"train_export.csv\", index = False)\n",
    "test_df.to_csv(\"test_export.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
